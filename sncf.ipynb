{
    "nbformat": 4, 
    "metadata": {
        "language_info": {
            "pygments_lexer": "scala", 
            "name": "scala", 
            "file_extension": ".scala", 
            "mimetype": "text/x-scala", 
            "codemirror_mode": "text/x-scala", 
            "version": "2.11.8"
        }, 
        "kernelspec": {
            "name": "scala-spark21", 
            "language": "scala", 
            "display_name": "Scala 2.11 with Spark 2.1"
        }
    }, 
    "cells": [
        {
            "outputs": [], 
            "source": "sc", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "sc.version", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# positions-geographiques-des-stations-du-reseau-ratp", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "+-------+--------------------+--------------------+--------------------+------------------+------------------+----------+-----------+\n|stop_id|           stop_name|           stop_desc|               coord|          stop_lat|          stop_lon|code_INSEE|departement|\n+-------+--------------------+--------------------+--------------------+------------------+------------------+----------+-----------+\n|   2158|       Ach\u00e8res-Ville|Avenue de Conflan...|48.9700771763, 2....|48.970077176304514|2.0776181820083806|     78005|         78|\n|   2159|              Al\u00e9sia|Place Victor et H...|48.8280660197, 2....| 48.82806601968645| 2.326827420050836|     75114|         75|\n|   2172|            Concorde|Tuileries (jardin...|48.8654893909, 2....|  48.8654893908901| 2.321411789213801|     75108|         75|\n|   2174|          Convention|Vaugirard (337 ru...|48.8371369496, 2....| 48.83713694956265| 2.296396090155559|     75115|         75|\n|   2178|Courcelle-sur-Yvette|Rue Fernand Leger...|48.7007630181, 2....| 48.70076301806364| 2.099100527058702|     91272|         91|\n+-------+--------------------+--------------------+--------------------+------------------+------------------+----------+-----------+\nonly showing top 5 rows\n\n", 
                    "output_type": "stream"
                }
            ], 
            "source": "// The code was removed by DSX for sharing.", 
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Validatations", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\n|      JOUR|CODE_STIF_TRNS|CODE_STIF_RES|CODE_STIF_ARRET|       LIBELLE_ARRET|ID_REFA_LDA|CATEGORIE_TITRE|NB_VALD|\n+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\n|2016-01-05|           100|          110|            762|SAINT-DENIS-PORTE...|      72285|      AMETHYSTE|    264|\n|2016-01-05|           100|          110|            766|       SAINT-GEORGES|      71366|            TST|    103|\n|2016-01-05|           100|          110|            768|       SAINT-JACQUES|      71041|            TST|    415|\n|2016-01-05|           100|          110|            769|        SAINT-LAZARE|      71370|      IMAGINE R|  15406|\n|2016-01-05|           100|          110|            777|SAINT-PAUL (LE MA...|      71222|    AUTRE TITRE|     88|\n+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\nonly showing top 5 rows\n\n", 
                    "output_type": "stream"
                }
            ], 
            "source": "import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types._\n\nval customSchema = StructType(Array(\n    StructField(\"JOUR\", DateType, true),\n    StructField(\"CODE_STIF_TRNS\", StringType, true),\n    StructField(\"CODE_STIF_RES\", StringType, true),\n    StructField(\"CODE_STIF_ARRET\", StringType, true),\n    StructField(\"LIBELLE_ARRET\", StringType, true),\n    StructField(\"ID_REFA_LDA\", StringType, true),\n    StructField(\"CATEGORIE_TITRE\", StringType, true),\n    StructField(\"NB_VALD\", IntegerType, true)))\n\nval dfValidation = spark.\n    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n    option(\"header\", \"true\").\n    schema(customSchema).\n    option(\"delimiter\",\";\").\n    option(\"nullValue\", \"null\").\n    load(\"swift://sncf.\" + name + \"/validations-sur-le-reseau-ferre-nombre-de-validations-par-jour-1er-semestre-2015.csv\")\ndfValidation.show(5)\n", 
            "execution_count": 56, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## 1. Total, min, max, \u00e9cart type de validations sur l'ensemble du r\u00e9seau", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "root\n |-- JOUR: date (nullable = true)\n |-- CODE_STIF_TRNS: string (nullable = true)\n |-- CODE_STIF_RES: string (nullable = true)\n |-- CODE_STIF_ARRET: string (nullable = true)\n |-- LIBELLE_ARRET: string (nullable = true)\n |-- ID_REFA_LDA: string (nullable = true)\n |-- CATEGORIE_TITRE: string (nullable = true)\n |-- NB_VALD: integer (nullable = true)\n\n", 
                    "output_type": "stream"
                }
            ], 
            "source": "dfValidation.printSchema()", 
            "execution_count": 47, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "+---------------+---------+-------+\n|       col_name|data_type|comment|\n+---------------+---------+-------+\n|           JOUR|     date|   null|\n| CODE_STIF_TRNS|   string|   null|\n|  CODE_STIF_RES|   string|   null|\n|CODE_STIF_ARRET|   string|   null|\n|  LIBELLE_ARRET|   string|   null|\n|    ID_REFA_LDA|   string|   null|\n|CATEGORIE_TITRE|   string|   null|\n|        NB_VALD|      int|   null|\n+---------------+---------+-------+\n\n", 
                    "output_type": "stream"
                }
            ], 
            "source": "// Register the DataFrame as a global temporary view\ndfValidation.createOrReplaceTempView(\"validation\")\nspark.sql(\"desc validation\").show()\n", 
            "execution_count": 49, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 50, 
                    "data": {
                        "text/plain": "Name: org.apache.spark.SparkException\nMessage: Job aborted due to stage failure: Task 0 in stage 38.0 failed 10 times, most recent failure: Lost task 0.9 in stage 38.0 (TID 240, yp-spark-dal09-env5-0049, executor b960e61c-e829-4b0a-bdf5-48d516cf0e96): java.lang.NumberFormatException: For input string: \"Moins de 5\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:76)\n\tat java.lang.Integer.parseInt(Integer.java:592)\n\tat java.lang.Integer.parseInt(Integer.java:627)\n\tat scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:29)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$.castTo(CSVInferSchema.scala:250)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:125)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:94)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:167)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:166)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:102)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.lang.Thread.run(Thread.java:785)\n\nDriver stacktrace:\nStackTrace: \tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:76)\n\tat java.lang.Integer.parseInt(Integer.java:592)\n\tat java.lang.Integer.parseInt(Integer.java:627)\n\tat scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:29)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$.castTo(CSVInferSchema.scala:250)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:125)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:94)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:167)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:166)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:102)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.lang.Thread.run(Thread.java:785)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1430)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1429)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1429)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1657)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at java.lang.Thread.getStackTrace(Thread.java:1117)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1957)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1971)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:954)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:381)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:953)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)\n  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375)\n  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375)\n  at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2778)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2375)\n  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2351)\n  ... 188 elided\nCaused by: java.lang.NumberFormatException: For input string: \"Moins de 5\"\n  at java.lang.NumberFormatException.forInputString(NumberFormatException.java:76)\n  at java.lang.Integer.parseInt(Integer.java:592)\n  at java.lang.Integer.parseInt(Integer.java:627)\n  at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)\n  at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)\n  at org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$.castTo(CSVInferSchema.scala:250)\n  at org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:125)\n  at org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:94)\n  at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:167)\n  at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:166)\n  at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:102)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "spark.sql(\"select MAX(NB_VALD) from validation\").collect()\n", 
            "execution_count": 50, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": " val rdd = sc.parallelize(\n      List( (2012,\"Tesla\",\"S\"), (1997,\"Ford\",\"E350\"), (2015,\"Chevy\",\"Volt\"))\n  )\n  val sqlContext = new SQLContext(sc)\n\n  // this is used to implicitly convert an RDD to a DataFrame.\n  import sqlContext.implicits._\n\n  val dataframe = rdd.toDF()\n\n  dataframe.foreach(println)\n\n dataframe.map(row => {\n    val row1 = row.getAs[String](1)\n    val make = if (row1.toLowerCase == \"tesla\") \"S\" else row1\n    Row(row(0),make,row(2))\n  }).collect().foreach(println)\n\n//[2012,S,S]\n//[1997,Ford,E350]\n//[2015,Chevy,Volt]", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [], 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat_minor": 0
}