{
    "nbformat": 4, 
    "metadata": {
        "language_info": {
            "pygments_lexer": "scala", 
            "name": "scala", 
            "file_extension": ".scala", 
            "mimetype": "text/x-scala", 
            "codemirror_mode": "text/x-scala", 
            "version": "2.11.8"
        }, 
        "celltoolbar": "Raw Cell Format", 
        "kernelspec": {
            "name": "scala-spark21", 
            "language": "scala", 
            "display_name": "Scala 2.11 with Spark 2.1"
        }
    }, 
    "cells": [
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 1, 
                    "data": {
                        "text/plain": "org.apache.spark.SparkContext@a97002dc"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "sc", 
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 2, 
                    "data": {
                        "text/plain": "2.1.0"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "sc.version", 
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# positions-geographiques-des-stations-du-reseau-ratp", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "+-------+--------------------+--------------------+--------------------+------------------+------------------+----------+-----------+\n|stop_id|           stop_name|           stop_desc|               coord|          stop_lat|          stop_lon|code_INSEE|departement|\n+-------+--------------------+--------------------+--------------------+------------------+------------------+----------+-----------+\n|   2158|       Ach\u00e8res-Ville|Avenue de Conflan...|48.9700771763, 2....|48.970077176304514|2.0776181820083806|     78005|         78|\n|   2159|              Al\u00e9sia|Place Victor et H...|48.8280660197, 2....| 48.82806601968645| 2.326827420050836|     75114|         75|\n|   2172|            Concorde|Tuileries (jardin...|48.8654893909, 2....|  48.8654893908901| 2.321411789213801|     75108|         75|\n|   2174|          Convention|Vaugirard (337 ru...|48.8371369496, 2....| 48.83713694956265| 2.296396090155559|     75115|         75|\n|   2178|Courcelle-sur-Yvette|Rue Fernand Leger...|48.7007630181, 2....| 48.70076301806364| 2.099100527058702|     91272|         91|\n+-------+--------------------+--------------------+--------------------+------------------+------------------+----------+-----------+\nonly showing top 5 rows\n\n", 
                    "output_type": "stream"
                }
            ], 
            "source": "// The code was removed by DSX for sharing.", 
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Validatations", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "Tue Jan 05 00:00:00 CST 2016", 
                    "output_type": "stream"
                }
            ], 
            "source": "// TEST Date cast\nimport java.text.SimpleDateFormat\nval jourFormat = new SimpleDateFormat(\"yyyy-MM-dd\")\nprint(jourFormat.parse(\"2016-01-05\"))\n", 
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## \"Moins de 5\" using RDD", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "1\nMap(LIBELLE_ARRET -> SAINT-DENIS-PORTE DE PARIS, CODE_STIF_TRNS -> 100, CODE_STIF_ARRET -> 762, JOUR -> 2016-01-05, CODE_STIF_RES -> 110, NB_VALD -> 264, ID_REFA_LDA -> 72285, CATEGORIE_TITRE -> AMETHYSTE)\n+--------------------+--------------+---------------+----------+-------------+-------+-----------+---------------+\n|       LIBELLE_ARRET|CODE_STIF_TRNS|CODE_STIF_ARRET|      JOUR|CODE_STIF_RES|NB_VALD|ID_REFA_LDA|CATEGORIE_TITRE|\n+--------------------+--------------+---------------+----------+-------------+-------+-----------+---------------+\n|SAINT-DENIS-PORTE...|           100|            762|2016-01-05|          110|  264.0|      72285|      AMETHYSTE|\n|       SAINT-GEORGES|           100|            766|2016-01-05|          110|  103.0|      71366|            TST|\n|       SAINT-JACQUES|           100|            768|2016-01-05|          110|  415.0|      71041|            TST|\n|        SAINT-LAZARE|           100|            769|2016-01-05|          110|15406.0|      71370|      IMAGINE R|\n|SAINT-PAUL (LE MA...|           100|            777|2016-01-05|          110|   88.0|      71222|    AUTRE TITRE|\n+--------------------+--------------+---------------+----------+-------------+-------+-----------+---------------+\nonly showing top 5 rows\n\nroot\n |-- LIBELLE_ARRET: string (nullable = true)\n |-- CODE_STIF_TRNS: string (nullable = true)\n |-- CODE_STIF_ARRET: string (nullable = true)\n |-- JOUR: date (nullable = true)\n |-- CODE_STIF_RES: string (nullable = true)\n |-- NB_VALD: double (nullable = true)\n |-- ID_REFA_LDA: string (nullable = true)\n |-- CATEGORIE_TITRE: string (nullable = true)\n\n", 
                    "output_type": "stream"
                }
            ], 
            "source": "import java.sql.Date\nval rdd_csv = sc.textFile(\"swift://sncf.\" + name + \"/validations-sur-le-reseau-ferre-nombre-de-validations-par-jour-1er-semestre-2015.csv\")\nval headerAndRows = rdd_csv.map(line => line.split(\";\").map(_.trim))\nval header = headerAndRows.first\n// filter out header (eh. just check if the first val matches the first header name)\nval data = headerAndRows.filter(_(0) != header(0))\n// splits to map (header/value pairs)\nval maps = data.map(splits => header.zip(splits).toMap)\n// filter out the user \"me\"\nval result = maps.filter(map => map(\"NB_VALD\") != \"Moins de 5\")\n// print result\nresult.take(1).foreach(println)\n\nval columns=result.take(1).flatMap(a=>a.keys)\nval resu=result.map{value=>\n      val list=value.values.toList\n      (list(0),list(1),list(2),Date.valueOf(list(3)),list(4),list(5).toDouble,list(6),list(7))\n      }.toDF(columns:_*)\n\nresu.show(5)\nresu.printSchema()\n", 
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## \"Moins de 5\" using DataFrame", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\n|      JOUR|CODE_STIF_TRNS|CODE_STIF_RES|CODE_STIF_ARRET|       LIBELLE_ARRET|ID_REFA_LDA|CATEGORIE_TITRE|NB_VALD|\n+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\n|2016-01-05|           100|          110|            762|SAINT-DENIS-PORTE...|      72285|      AMETHYSTE|    264|\n|2016-01-05|           100|          110|            766|       SAINT-GEORGES|      71366|            TST|    103|\n|2016-01-05|           100|          110|            768|       SAINT-JACQUES|      71041|            TST|    415|\n|2016-01-05|           100|          110|            769|        SAINT-LAZARE|      71370|      IMAGINE R|  15406|\n|2016-01-05|           100|          110|            777|SAINT-PAUL (LE MA...|      71222|    AUTRE TITRE|     88|\n+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\nonly showing top 5 rows\n\nroot\n |-- JOUR: date (nullable = true)\n |-- CODE_STIF_TRNS: string (nullable = true)\n |-- CODE_STIF_RES: string (nullable = true)\n |-- CODE_STIF_ARRET: string (nullable = true)\n |-- LIBELLE_ARRET: string (nullable = true)\n |-- ID_REFA_LDA: string (nullable = true)\n |-- CATEGORIE_TITRE: string (nullable = true)\n |-- NB_VALD: integer (nullable = true)\n\n", 
                    "output_type": "stream"
                }
            ], 
            "source": "import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types._\n\nval customSchema = StructType(Array(\n    StructField(\"JOUR\", DateType, true),\n    StructField(\"CODE_STIF_TRNS\", StringType, true),\n    StructField(\"CODE_STIF_RES\", StringType, true),\n    StructField(\"CODE_STIF_ARRET\", StringType, true),\n    StructField(\"LIBELLE_ARRET\", StringType, true),\n    StructField(\"ID_REFA_LDA\", StringType, true),\n    StructField(\"CATEGORIE_TITRE\", StringType, true),\n    StructField(\"NB_VALD\", IntegerType, true)))\n//-- Important : DROPMALFORMED\nval dfValidation = spark.\n    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n    option(\"header\", \"true\").\n    schema(customSchema).\n    option(\"delimiter\",\";\").\n    option(\"nullValue\", \"null\").\n    option(\"mode\", \"DROPMALFORMED\").\n    load(\"swift://sncf.\" + name + \"/validations-sur-le-reseau-ferre-nombre-de-validations-par-jour-1er-semestre-2015.csv\")\ndfValidation.show(5)\ndfValidation.printSchema()", 
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## 1. Total, min, max, \u00e9cart type de validations sur l'ensemble du r\u00e9seau", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "+---------------+---------+-------+\n|       col_name|data_type|comment|\n+---------------+---------+-------+\n|           JOUR|     date|   null|\n| CODE_STIF_TRNS|   string|   null|\n|  CODE_STIF_RES|   string|   null|\n|CODE_STIF_ARRET|   string|   null|\n|  LIBELLE_ARRET|   string|   null|\n|    ID_REFA_LDA|   string|   null|\n|CATEGORIE_TITRE|   string|   null|\n|        NB_VALD|      int|   null|\n+---------------+---------+-------+\n\n+------+---+---------+-----------------+\n|   max|min|      sum|              std|\n+------+---+---------+-----------------+\n|117496|  5|839349975|3340.326185063345|\n+------+---+---------+-----------------+\n\n", 
                    "output_type": "stream"
                }
            ], 
            "source": "// Register the DataFrame as a global temporary view\ndfValidation.createOrReplaceTempView(\"validation\")\nspark.sql(\"desc validation\").show()\nval exo1=spark.sql(\"select MAX(NB_VALD) as max ,MIN(NB_VALD) as min,SUM(NB_VALD) as sum ,STD(NB_VALD) as std from validation\")\nexo1.show()\n", 
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "+---------------+---------+-------+\n|       col_name|data_type|comment|\n+---------------+---------+-------+\n|  LIBELLE_ARRET|   string|   null|\n| CODE_STIF_TRNS|   string|   null|\n|CODE_STIF_ARRET|   string|   null|\n|           JOUR|     date|   null|\n|  CODE_STIF_RES|   string|   null|\n|        NB_VALD|   double|   null|\n|    ID_REFA_LDA|   string|   null|\n|CATEGORIE_TITRE|   string|   null|\n+---------------+---------+-------+\n\n", 
                    "output_type": "stream"
                }
            ], 
            "source": "// Register the DataFrame as a global temporary view\nresu.createOrReplaceTempView(\"validation\")\nspark.sql(\"desc validation\").show()\n", 
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "+--------+---+------------+------------------+\n|     max|min|         sum|               std|\n+--------+---+------------+------------------+\n|117496.0|5.0|8.39349975E8|3340.3261850633626|\n+--------+---+------------+------------------+\n\n", 
                    "output_type": "stream"
                }
            ], 
            "source": "val exo1=spark.sql(\"select MAX(NB_VALD) as max ,MIN(NB_VALD) as min,SUM(NB_VALD) as sum ,STD(NB_VALD) as std from validation\")\nexo1.show()", 
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat_minor": 0
}