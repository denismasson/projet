{
    "metadata": {
        "kernelspec": {
            "language": "scala", 
            "display_name": "Scala 2.11 with Spark 2.1", 
            "name": "scala-spark21"
        }, 
        "celltoolbar": "Raw Cell Format", 
        "language_info": {
            "name": "scala", 
            "codemirror_mode": "text/x-scala", 
            "version": "2.11.8", 
            "mimetype": "text/x-scala", 
            "pygments_lexer": "scala", 
            "file_extension": ".scala"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "execution_count": 1, 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 1, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "org.apache.spark.SparkContext@a97002dc"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "sc"
        }, 
        {
            "execution_count": 2, 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 2, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "2.1.0"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "sc.version"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "# positions-geographiques-des-stations-du-reseau-ratp"
        }, 
        {
            "execution_count": 3, 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+--------------------+--------------------+--------------------+------------------+------------------+----------+-----------+\n|stop_id|           stop_name|           stop_desc|               coord|          stop_lat|          stop_lon|code_INSEE|departement|\n+-------+--------------------+--------------------+--------------------+------------------+------------------+----------+-----------+\n|   2158|       Ach\u00e8res-Ville|Avenue de Conflan...|48.9700771763, 2....|48.970077176304514|2.0776181820083806|     78005|         78|\n|   2159|              Al\u00e9sia|Place Victor et H...|48.8280660197, 2....| 48.82806601968645| 2.326827420050836|     75114|         75|\n|   2172|            Concorde|Tuileries (jardin...|48.8654893909, 2....|  48.8654893908901| 2.321411789213801|     75108|         75|\n|   2174|          Convention|Vaugirard (337 ru...|48.8371369496, 2....| 48.83713694956265| 2.296396090155559|     75115|         75|\n|   2178|Courcelle-sur-Yvette|Rue Fernand Leger...|48.7007630181, 2....| 48.70076301806364| 2.099100527058702|     91272|         91|\n+-------+--------------------+--------------------+--------------------+------------------+------------------+----------+-----------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "// The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": ""
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "# Validatations"
        }, 
        {
            "execution_count": 4, 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Tue Jan 05 00:00:00 CST 2016"
                }
            ], 
            "source": "// TEST Date cast\nimport java.text.SimpleDateFormat\nval jourFormat = new SimpleDateFormat(\"yyyy-MM-dd\")\nprint(jourFormat.parse(\"2016-01-05\"))\n"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## \"Moins de 5\" using RDD"
        }, 
        {
            "execution_count": 11, 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "1\nMap(LIBELLE_ARRET -> SAINT-DENIS-PORTE DE PARIS, CODE_STIF_TRNS -> 100, CODE_STIF_ARRET -> 762, JOUR -> 2016-01-05, CODE_STIF_RES -> 110, NB_VALD -> 264, ID_REFA_LDA -> 72285, CATEGORIE_TITRE -> AMETHYSTE)\n+--------------------+--------------+---------------+----------+-------------+-------+-----------+---------------+\n|       LIBELLE_ARRET|CODE_STIF_TRNS|CODE_STIF_ARRET|      JOUR|CODE_STIF_RES|NB_VALD|ID_REFA_LDA|CATEGORIE_TITRE|\n+--------------------+--------------+---------------+----------+-------------+-------+-----------+---------------+\n|SAINT-DENIS-PORTE...|           100|            762|2016-01-05|          110|  264.0|      72285|      AMETHYSTE|\n|       SAINT-GEORGES|           100|            766|2016-01-05|          110|  103.0|      71366|            TST|\n|       SAINT-JACQUES|           100|            768|2016-01-05|          110|  415.0|      71041|            TST|\n|        SAINT-LAZARE|           100|            769|2016-01-05|          110|15406.0|      71370|      IMAGINE R|\n|SAINT-PAUL (LE MA...|           100|            777|2016-01-05|          110|   88.0|      71222|    AUTRE TITRE|\n+--------------------+--------------+---------------+----------+-------------+-------+-----------+---------------+\nonly showing top 5 rows\n\nroot\n |-- LIBELLE_ARRET: string (nullable = true)\n |-- CODE_STIF_TRNS: string (nullable = true)\n |-- CODE_STIF_ARRET: string (nullable = true)\n |-- JOUR: date (nullable = true)\n |-- CODE_STIF_RES: string (nullable = true)\n |-- NB_VALD: double (nullable = true)\n |-- ID_REFA_LDA: string (nullable = true)\n |-- CATEGORIE_TITRE: string (nullable = true)\n\n"
                }
            ], 
            "source": "import java.sql.Date\nval rdd_csv = sc.textFile(\"swift://sncf.\" + name + \"/validations-sur-le-reseau-ferre-nombre-de-validations-par-jour-1er-semestre-2015.csv\")\nval headerAndRows = rdd_csv.map(line => line.split(\";\").map(_.trim))\nval header = headerAndRows.first\n// filter out header (eh. just check if the first val matches the first header name)\nval data = headerAndRows.filter(_(0) != header(0))\n// splits to map (header/value pairs)\nval maps = data.map(splits => header.zip(splits).toMap)\n// filter out the NB_VALD \"Moins de 5\"\nval result = maps.filter(map => map(\"NB_VALD\") != \"Moins de 5\")\n// print result\nresult.take(1).foreach(println)\n\nval columns=result.take(1).flatMap(a=>a.keys)\nval resu=result.map{value=>\n      val list=value.values.toList\n      (list(0),list(1),list(2),Date.valueOf(list(3)),list(4),list(5).toDouble,list(6),list(7))\n      }.toDF(columns:_*)\n\nresu.show(5)\nresu.printSchema()\n"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## \"Moins de 5\" using DataFrame"
        }, 
        {
            "execution_count": 12, 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\n|      JOUR|CODE_STIF_TRNS|CODE_STIF_RES|CODE_STIF_ARRET|       LIBELLE_ARRET|ID_REFA_LDA|CATEGORIE_TITRE|NB_VALD|\n+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\n|2016-01-05|           100|          110|            762|SAINT-DENIS-PORTE...|      72285|      AMETHYSTE|    264|\n|2016-01-05|           100|          110|            766|       SAINT-GEORGES|      71366|            TST|    103|\n|2016-01-05|           100|          110|            768|       SAINT-JACQUES|      71041|            TST|    415|\n|2016-01-05|           100|          110|            769|        SAINT-LAZARE|      71370|      IMAGINE R|  15406|\n|2016-01-05|           100|          110|            777|SAINT-PAUL (LE MA...|      71222|    AUTRE TITRE|     88|\n+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\nonly showing top 5 rows\n\nroot\n |-- JOUR: date (nullable = true)\n |-- CODE_STIF_TRNS: string (nullable = true)\n |-- CODE_STIF_RES: string (nullable = true)\n |-- CODE_STIF_ARRET: string (nullable = true)\n |-- LIBELLE_ARRET: string (nullable = true)\n |-- ID_REFA_LDA: string (nullable = true)\n |-- CATEGORIE_TITRE: string (nullable = true)\n |-- NB_VALD: integer (nullable = true)\n\n"
                }
            ], 
            "source": "import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types._\n\nval customSchema = StructType(Array(\n    StructField(\"JOUR\", DateType, true),\n    StructField(\"CODE_STIF_TRNS\", StringType, true),\n    StructField(\"CODE_STIF_RES\", StringType, true),\n    StructField(\"CODE_STIF_ARRET\", StringType, true),\n    StructField(\"LIBELLE_ARRET\", StringType, true),\n    StructField(\"ID_REFA_LDA\", StringType, true),\n    StructField(\"CATEGORIE_TITRE\", StringType, true),\n    StructField(\"NB_VALD\", IntegerType, true)))\n//-- Important : DROPMALFORMED\nval dfValidation = spark.\n    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n    option(\"header\", \"true\").\n    schema(customSchema).\n    option(\"delimiter\",\";\").\n    option(\"nullValue\", \"null\").\n    option(\"mode\", \"DROPMALFORMED\").\n    load(\"swift://sncf.\" + name + \"/validations-sur-le-reseau-ferre-nombre-de-validations-par-jour-1er-semestre-2015.csv\")\ndfValidation.show(5)\ndfValidation.printSchema()"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown", 
            "source": "## 1. Total, min, max, \u00e9cart type de validations sur l'ensemble du r\u00e9seau"
        }, 
        {
            "execution_count": 18, 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---------------+---------+-------+\n|       col_name|data_type|comment|\n+---------------+---------+-------+\n|           JOUR|     date|   null|\n| CODE_STIF_TRNS|   string|   null|\n|  CODE_STIF_RES|   string|   null|\n|CODE_STIF_ARRET|   string|   null|\n|  LIBELLE_ARRET|   string|   null|\n|    ID_REFA_LDA|   string|   null|\n|CATEGORIE_TITRE|   string|   null|\n|        NB_VALD|      int|   null|\n+---------------+---------+-------+\n\n+------+---+---------+-----------------+\n|   max|min|      sum|              std|\n+------+---+---------+-----------------+\n|117496|  5|839349975|3340.326185063387|\n+------+---+---------+-----------------+\n\n"
                }
            ], 
            "source": "// Register the DataFrame as a global temporary view\ndfValidation.createOrReplaceTempView(\"validation\")\nspark.sql(\"desc validation\").show()\nval exo1=spark.sql(\"select MAX(NB_VALD) as max ,MIN(NB_VALD) as min,SUM(NB_VALD) as sum ,STD(NB_VALD) as std from validation\")\nexo1.show()\n"
        }, 
        {
            "execution_count": 14, 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---------------+---------+-------+\n|       col_name|data_type|comment|\n+---------------+---------+-------+\n|  LIBELLE_ARRET|   string|   null|\n| CODE_STIF_TRNS|   string|   null|\n|CODE_STIF_ARRET|   string|   null|\n|           JOUR|     date|   null|\n|  CODE_STIF_RES|   string|   null|\n|        NB_VALD|   double|   null|\n|    ID_REFA_LDA|   string|   null|\n|CATEGORIE_TITRE|   string|   null|\n+---------------+---------+-------+\n\n"
                }
            ], 
            "source": "// Register the DataFrame as a global temporary view\nresu.createOrReplaceTempView(\"validation\")\nspark.sql(\"desc validation\").show()\n"
        }, 
        {
            "execution_count": 15, 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+--------+---+------------+------------------+\n|     max|min|         sum|               std|\n+--------+---+------------+------------------+\n|117496.0|5.0|8.39349975E8|3340.3261850633626|\n+--------+---+------------+------------------+\n\n"
                }
            ], 
            "source": "val exo1=spark.sql(\"select MAX(NB_VALD) as max ,MIN(NB_VALD) as min,SUM(NB_VALD) as sum ,STD(NB_VALD) as std from validation\")\nexo1.show()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 2. Total, min, max, \u00e9cart type de validations sur l'ensemble du r\u00e9seau\u00b6"
        }, 
        {
            "execution_count": 19, 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---------------+------+---+---------+------------------+\n|CATEGORIE_TITRE|   max|min|      sum|               std|\n+---------------+------+---+---------+------------------+\n|         NAVIGO|117496|  5|566037981|6741.0084156052835|\n|     NON DEFINI|    14|  5|      439|1.8580651587930026|\n|      IMAGINE R| 19848|  5|137940120|1432.1276500195768|\n|            FGT|  6262|  5| 36333147|424.46848008771155|\n|    AUTRE TITRE|  6961|  5| 15030393|347.00778001998583|\n|            TST| 13332|  5| 63756820| 804.7275485319755|\n|      AMETHYSTE|  2632|  5| 20251075|208.15668474338432|\n+---------------+------+---+---------+------------------+\n\n"
                }
            ], 
            "source": "val exo2=spark.sql(\"select CATEGORIE_TITRE, MAX(NB_VALD) as max ,MIN(NB_VALD) as min,SUM(NB_VALD) as sum ,STD(NB_VALD) as std from validation group by CATEGORIE_TITRE\")\nexo2.show()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 3. Total, min, max, \u00e9cart type de validations par station"
        }, 
        {
            "execution_count": 20, 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+--------------------+-----+---+-------+------------------+\n|       LIBELLE_ARRET|  max|min|    sum|               std|\n+--------------------+-----+---+-------+------------------+\n|                EGLY|  376|  5|  61544| 108.6159953329052|\n|   CENSIER-DAUBENTON| 6381|  7|1401786|1828.7771316737378|\n|      LAGNY-THORIGNY| 4791| 17| 764461| 1087.679400414759|\n|           SAINT-CYR| 3460| 11| 617425| 890.6392464580025|\n|               PASSY| 7840|  5|1472529|2051.0996447084526|\n|     GRAVIGNY-BALIZY|  464|  5|  69129|127.58094482342993|\n|PONT-NEUF (LA MON...| 3404| 13| 460452| 658.8047639668467|\n|  PORTE DE CHARENTON| 4509| 18| 962686|1194.3798961405444|\n|           GARIBALDI| 7181| 10|1435081|1789.4180059677938|\n|             TAVERNY|  847|  5| 157469|218.68868034448818|\n|              HERICY|   45|  5|   4829|10.644892604990504|\n|                OSNY|  338|  5|  42528| 84.07423890518126|\n|    MAIRIE DE CLICHY|15588| 31|2979463|4376.6448721747065|\n|   PRE SAINT-GERVAIS|  720| 13| 154165|177.55012431575742|\n|    CERGY-PREFECTURE|10117|  5|2238055|   2890.7993254862|\n|     ERMONT-EAUBONNE| 8880| 28|1611726|2242.1890422534934|\n|     ECOLE MILITAIRE|10480| 22|1373212| 2105.784636220763|\n|PARC DES EXPOSITIONS| 6322|  6| 618859| 1036.082184137561|\n|CHARLES DE GAULLE...|28073| 54|5578152| 4738.839458553422|\n|              ANVERS| 8048| 20|1698023|2174.2155165732993|\n+--------------------+-----+---+-------+------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "val exo3=spark.sql(\"select LIBELLE_ARRET, MAX(NB_VALD) as max ,MIN(NB_VALD) as min,SUM(NB_VALD) as sum ,STD(NB_VALD) as std from validation group by LIBELLE_ARRET\")\nexo3.show()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 4. Total, min, max, \u00e9cart type de validations par mois"
        }, 
        {
            "execution_count": 22, 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----------+------+---+---------+------------------+\n|month(JOUR)|   max|min|      sum|               std|\n+-----------+------+---+---------+------------------+\n|          1|117496|  5|140726652|  3328.18187446804|\n|          2|109239|  5|138403148|3367.7334658128448|\n|          3|108998|  5|145065114| 3322.785100693589|\n|          4|110219|  5|137832683| 3284.150133365392|\n|          5|110976|  5|137937966|3235.9367153657245|\n|          6|110287|  5|139384412|3505.9581337188856|\n+-----------+------+---+---------+------------------+\n\n"
                }
            ], 
            "source": "val exo4=spark.sql(\"select MONTH(JOUR), MAX(NB_VALD) as max ,MIN(NB_VALD) as min,SUM(NB_VALD) as sum ,STD(NB_VALD) as std from validation group by MONTH(JOUR) order by 1\")\nexo4.show()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 5. Jour de l'ann\u00e9e/Titre de transport/Station ayant le plus/le moins de validation (au total et par titre de transport)"
        }, 
        {
            "execution_count": 41, 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------+---------------+--------------------+-------+\n|      JOUR|CATEGORIE_TITRE|       LIBELLE_ARRET|NB_VALD|\n+----------+---------------+--------------------+-------+\n|2016-01-22|         NAVIGO|LA DEFENSE-GRANDE...| 117496|\n|2016-01-27|     NON DEFINI|BIBLIOTHEQUE FRAN...|     14|\n|2016-02-05|      IMAGINE R|        SAINT-LAZARE|  19848|\n|2016-03-12|            FGT|        GARE DU NORD|   6262|\n|2016-03-14|    AUTRE TITRE|        GARE DE LYON|   6961|\n|2016-03-12|            TST|        GARE DU NORD|  13332|\n|2016-05-03|      AMETHYSTE|          BELLEVILLE|   2632|\n+----------+---------------+--------------------+-------+\n\n"
                }
            ], 
            "source": "val exo5=spark.sql(\"select JOUR,CATEGORIE_TITRE, LIBELLE_ARRET,NB_VALD from ( select JOUR,CATEGORIE_TITRE, LIBELLE_ARRET,NB_VALD, dense_rank() OVER (PARTITION BY CATEGORIE_TITRE ORDER BY NB_VALD DESC) as rank FROM validation ) tmp WHERE rank = 1\")\nexo5.show()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 6. G\u00e9olocalisation des dix stations de m\u00e9tro ayant le plus/le moins de validations par titre de transport"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 7. Vous pouvez apporter une visualisation ou des indicateurs compl\u00e9mentaires"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "source": ""
        }
    ], 
    "nbformat_minor": 0
}