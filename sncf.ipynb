{
    "nbformat": 4, 
    "metadata": {
        "language_info": {
            "pygments_lexer": "scala", 
            "name": "scala", 
            "file_extension": ".scala", 
            "mimetype": "text/x-scala", 
            "codemirror_mode": "text/x-scala", 
            "version": "2.11.8"
        }, 
        "celltoolbar": "Raw Cell Format", 
        "kernelspec": {
            "name": "scala-spark21", 
            "language": "scala", 
            "display_name": "Scala 2.11 with Spark 2.1"
        }
    }, 
    "cells": [
        {
            "outputs": [], 
            "source": "sc", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "sc.version", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# positions-geographiques-des-stations-du-reseau-ratp", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "1\n", 
                    "output_type": "stream"
                }, 
                {
                    "output_type": "execute_result", 
                    "execution_count": 55, 
                    "data": {
                        "text/plain": "Name: Unknown Error\nMessage: <console>:85: error: not found: value display\n       display(dfGeo)\n       ^\nStackTrace: "
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "// The code was removed by DSX for sharing.", 
            "execution_count": 55, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 58, 
                    "data": {
                        "text/plain": "Name: Error parsing magics!\nMessage: Magics [display] do not exist!\nStackTrace: "
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "%display dfGeo", 
            "execution_count": 58, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Validatations", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "Tue Jan 05 00:00:00 CST 2016", 
                    "output_type": "stream"
                }
            ], 
            "source": "// TEST Date cast\nimport java.text.SimpleDateFormat\nval jourFormat = new SimpleDateFormat(\"yyyy-MM-dd\")\nprint(jourFormat.parse(\"2016-01-05\"))\n", 
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "Map(LIBELLE_ARRET -> SAINT-DENIS-PORTE DE PARIS, CODE_STIF_TRNS -> 100, CODE_STIF_ARRET -> 762, JOUR -> 2016-01-05, CODE_STIF_RES -> 110, NB_VALD -> 264, ID_REFA_LDA -> 72285, CATEGORIE_TITRE -> AMETHYSTE)\n+--------------------+--------------+---------------+----------+-------------+-------+-----------+---------------+\n|       LIBELLE_ARRET|CODE_STIF_TRNS|CODE_STIF_ARRET|      JOUR|CODE_STIF_RES|NB_VALD|ID_REFA_LDA|CATEGORIE_TITRE|\n+--------------------+--------------+---------------+----------+-------------+-------+-----------+---------------+\n|SAINT-DENIS-PORTE...|           100|            762|2016-01-05|          110|  264.0|      72285|      AMETHYSTE|\n|       SAINT-GEORGES|           100|            766|2016-01-05|          110|  103.0|      71366|            TST|\n|       SAINT-JACQUES|           100|            768|2016-01-05|          110|  415.0|      71041|            TST|\n|        SAINT-LAZARE|           100|            769|2016-01-05|          110|15406.0|      71370|      IMAGINE R|\n|SAINT-PAUL (LE MA...|           100|            777|2016-01-05|          110|   88.0|      71222|    AUTRE TITRE|\n|SAINT-PHILIPPE-DU...|           100|            778|2016-01-05|          110|  286.0|      71334|            TST|\n|       SAINT-PLACIDE|           100|            779|2016-01-05|          110|  242.0|      71184|            FGT|\n|       SAINT-PLACIDE|           100|            779|2016-01-05|          110| 5786.0|      71184|         NAVIGO|\n|       SAINT-SULPICE|           100|            781|2016-01-05|          110|   43.0|      71202|    AUTRE TITRE|\n|       SAINT-SULPICE|           100|            781|2016-01-05|          110| 1314.0|      71202|      IMAGINE R|\n|               SEGUR|           100|            791|2016-01-05|          110| 2996.0|      71157|         NAVIGO|\n|             SENTIER|           100|            792|2016-01-05|          110|   82.0|      73634|    AUTRE TITRE|\n|BOBIGNY-PANTIN (R...|           100|             80|2016-01-05|          110|   80.0|      72039|    AUTRE TITRE|\n|STRASBOURG-SAINT-...|           100|            836|2016-01-05|          110|  294.0|      73633|    AUTRE TITRE|\n|STRASBOURG-SAINT-...|           100|            836|2016-01-05|          110| 2570.0|      73633|            TST|\n|          TELEGRAPHE|           100|            848|2016-01-05|          110|  955.0|      71870|      IMAGINE R|\n|              TEMPLE|           100|            849|2016-01-05|          110|  511.0|      71296|      IMAGINE R|\n|              TEMPLE|           100|            849|2016-01-05|          110| 2092.0|      71296|         NAVIGO|\n|              TERNES|           100|            850|2016-01-05|          110|   36.0|      71367|    AUTRE TITRE|\n|              TERNES|           100|            850|2016-01-05|          110| 1233.0|      71367|      IMAGINE R|\n+--------------------+--------------+---------------+----------+-------------+-------+-----------+---------------+\nonly showing top 20 rows\n\nroot\n |-- LIBELLE_ARRET: string (nullable = true)\n |-- CODE_STIF_TRNS: string (nullable = true)\n |-- CODE_STIF_ARRET: string (nullable = true)\n |-- JOUR: date (nullable = true)\n |-- CODE_STIF_RES: string (nullable = true)\n |-- NB_VALD: double (nullable = true)\n |-- ID_REFA_LDA: string (nullable = true)\n |-- CATEGORIE_TITRE: string (nullable = true)\n\n", 
                    "output_type": "stream"
                }, 
                {
                    "output_type": "execute_result", 
                    "execution_count": 1, 
                    "data": {
                        "text/plain": "Name: Unknown Error\nMessage: <console>:238: error: not found: value df3\n       df3.printSchema()\n       ^\nStackTrace: "
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "import java.sql.Date\nval rdd_csv = sc.textFile(\"swift://sncf.\" + name + \"/validations-sur-le-reseau-ferre-nombre-de-validations-par-jour-1er-semestre-2015.csv\")\nval headerAndRows = rdd_csv.map(line => line.split(\";\").map(_.trim))\n// filter out header (eh. just check if the first val matches the first header name)\nval data = headerAndRows.filter(_(0) != header(0))\n// splits to map (header/value pairs)\nval maps = data.map(splits => header.zip(splits).toMap)\n// filter out the user \"me\"\nval result = maps.filter(map => map(\"NB_VALD\") != \"Moins de 5\")\n// print result\nresult.take(1).foreach(println)\n\nval columns=result.take(1).flatMap(a=>a.keys)\nval resu=result.map{value=>\n      val list=value.values.toList\n      (list(0),list(1),list(2),Date.valueOf(list(3)),list(4),list(5).toDouble,list(6),list(7))\n      }.toDF(columns:_*)\n\nresu.show()\nresu.printSchema()\n", 
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 7, 
                    "data": {
                        "text/plain": "Name: Unknown Error\nMessage: <console>:249: error: overloaded method value createDataset with alternatives:\n  [T](data: java.util.List[T])(implicit evidence$6: org.apache.spark.sql.Encoder[T])org.apache.spark.sql.Dataset[T] <and>\n  [T](data: org.apache.spark.rdd.RDD[T])(implicit evidence$5: org.apache.spark.sql.Encoder[T])org.apache.spark.sql.Dataset[T] <and>\n  [T](data: Seq[T])(implicit evidence$4: org.apache.spark.sql.Encoder[T])org.apache.spark.sql.Dataset[T]\n cannot be applied to (org.apache.spark.sql.DataFrame)\n       val ds = spark.createDataset(resultantDF)\n                      ^\nStackTrace: "
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "import spark.implicits._\nval ds = spark.createDataset(resultantDF)", 
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "// create DataFrame from RDD (Programmatically Specifying the Schema) \nval headerColumns = rdd.first().split(\",\").to[List] \n// extract headers [..] first\ndef dfSchema(columnNames: List[String]): StructType = { StructType( Seq( StructField(name = \"manager_name\", dataType = StringType, nullable = false),\n                                                                        StructField(name = \"client_name\", dataType = StringType, nullable = false),\n                                                                        StructField(name = \"client_gender\", dataType = StringType, nullable = false),\n                                                                        StructField(name = \"client_age\", dataType = IntegerType, nullable = false),\n                                                                        StructField(name = \"response_time\", dataType = DoubleType, nullable = false),\n                                                                        StructField(name = \"satisfaction_level\", dataType = DoubleType, nullable = fals) ) ) } \n// create a data row \ndef row(line: List[String]): Row = { Row(line(0), line(1), line(2), line(3).toInt, line(4).toDouble, line(5).toDouble) } \n// define a schema for the file \nval schema = dfSchema(headerColumns) val data = rdd .mapPartitionsWithIndex((index, element) => if (index == 0) it.drop(1) else it) \n// skip header \n.map(_.split(\",\").to[List]) .map(row) \nval dataFrame = spark.createDataFrame(data, schema)\n", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\n|      JOUR|CODE_STIF_TRNS|CODE_STIF_RES|CODE_STIF_ARRET|       LIBELLE_ARRET|ID_REFA_LDA|CATEGORIE_TITRE|NB_VALD|\n+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\n|2016-01-05|           100|          110|            762|SAINT-DENIS-PORTE...|      72285|      AMETHYSTE|    264|\n|2016-01-05|           100|          110|            766|       SAINT-GEORGES|      71366|            TST|    103|\n|2016-01-05|           100|          110|            768|       SAINT-JACQUES|      71041|            TST|    415|\n|2016-01-05|           100|          110|            769|        SAINT-LAZARE|      71370|      IMAGINE R|  15406|\n|2016-01-05|           100|          110|            777|SAINT-PAUL (LE MA...|      71222|    AUTRE TITRE|     88|\n+----------+--------------+-------------+---------------+--------------------+-----------+---------------+-------+\nonly showing top 5 rows\n\n", 
                    "output_type": "stream"
                }
            ], 
            "source": "import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types._\n\nval customSchema = StructType(Array(\n    StructField(\"JOUR\", DateType, true),\n    StructField(\"CODE_STIF_TRNS\", StringType, true),\n    StructField(\"CODE_STIF_RES\", StringType, true),\n    StructField(\"CODE_STIF_ARRET\", StringType, true),\n    StructField(\"LIBELLE_ARRET\", StringType, true),\n    StructField(\"ID_REFA_LDA\", StringType, true),\n    StructField(\"CATEGORIE_TITRE\", StringType, true),\n    StructField(\"NB_VALD\", IntegerType, true)))\n\nval dfValidation = spark.\n    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n    option(\"header\", \"true\").\n    schema(customSchema).\n    option(\"delimiter\",\";\").\n    option(\"nullValue\", \"null\").\n    load(\"swift://sncf.\" + name + \"/validations-sur-le-reseau-ferre-nombre-de-validations-par-jour-1er-semestre-2015.csv\")\ndfValidation.show(5)\n", 
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## 1. Total, min, max, \u00e9cart type de validations sur l'ensemble du r\u00e9seau", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "root\n |-- JOUR: date (nullable = true)\n |-- CODE_STIF_TRNS: string (nullable = true)\n |-- CODE_STIF_RES: string (nullable = true)\n |-- CODE_STIF_ARRET: string (nullable = true)\n |-- LIBELLE_ARRET: string (nullable = true)\n |-- ID_REFA_LDA: string (nullable = true)\n |-- CATEGORIE_TITRE: string (nullable = true)\n |-- NB_VALD: integer (nullable = true)\n\n", 
                    "output_type": "stream"
                }
            ], 
            "source": "dfValidation.printSchema()", 
            "execution_count": 47, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "+---------------+---------+-------+\n|       col_name|data_type|comment|\n+---------------+---------+-------+\n|           JOUR|     date|   null|\n| CODE_STIF_TRNS|   string|   null|\n|  CODE_STIF_RES|   string|   null|\n|CODE_STIF_ARRET|   string|   null|\n|  LIBELLE_ARRET|   string|   null|\n|    ID_REFA_LDA|   string|   null|\n|CATEGORIE_TITRE|   string|   null|\n|        NB_VALD|      int|   null|\n+---------------+---------+-------+\n\n", 
                    "output_type": "stream"
                }
            ], 
            "source": "// Register the DataFrame as a global temporary view\ndfValidation.createOrReplaceTempView(\"validation\")\nspark.sql(\"desc validation\").show()\n", 
            "execution_count": 49, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 50, 
                    "data": {
                        "text/plain": "Name: org.apache.spark.SparkException\nMessage: Job aborted due to stage failure: Task 0 in stage 38.0 failed 10 times, most recent failure: Lost task 0.9 in stage 38.0 (TID 240, yp-spark-dal09-env5-0049, executor b960e61c-e829-4b0a-bdf5-48d516cf0e96): java.lang.NumberFormatException: For input string: \"Moins de 5\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:76)\n\tat java.lang.Integer.parseInt(Integer.java:592)\n\tat java.lang.Integer.parseInt(Integer.java:627)\n\tat scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:29)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$.castTo(CSVInferSchema.scala:250)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:125)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:94)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:167)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:166)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:102)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.lang.Thread.run(Thread.java:785)\n\nDriver stacktrace:\nStackTrace: \tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:76)\n\tat java.lang.Integer.parseInt(Integer.java:592)\n\tat java.lang.Integer.parseInt(Integer.java:627)\n\tat scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:29)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$.castTo(CSVInferSchema.scala:250)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:125)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:94)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:167)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:166)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:102)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.lang.Thread.run(Thread.java:785)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1430)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1429)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1429)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1657)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at java.lang.Thread.getStackTrace(Thread.java:1117)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1957)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1971)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:954)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:381)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:953)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)\n  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375)\n  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375)\n  at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2778)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2375)\n  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2351)\n  ... 188 elided\nCaused by: java.lang.NumberFormatException: For input string: \"Moins de 5\"\n  at java.lang.NumberFormatException.forInputString(NumberFormatException.java:76)\n  at java.lang.Integer.parseInt(Integer.java:592)\n  at java.lang.Integer.parseInt(Integer.java:627)\n  at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)\n  at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)\n  at org.apache.spark.sql.execution.datasources.csv.CSVTypeCast$.castTo(CSVInferSchema.scala:250)\n  at org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:125)\n  at org.apache.spark.sql.execution.datasources.csv.CSVRelation$$anonfun$csvParser$3.apply(CSVRelation.scala:94)\n  at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:167)\n  at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$1$$anonfun$apply$2.apply(CSVFileFormat.scala:166)\n  at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:102)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "spark.sql(\"select MAX(NB_VALD) from validation\").collect()\n", 
            "execution_count": 50, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": " val rdd = sc.parallelize(\n      List( (2012,\"Tesla\",\"S\"), (1997,\"Ford\",\"E350\"), (2015,\"Chevy\",\"Volt\"))\n  )\n  val sqlContext = new SQLContext(sc)\n\n  // this is used to implicitly convert an RDD to a DataFrame.\n  import sqlContext.implicits._\n\n  val dataframe = rdd.toDF()\n\n  dataframe.foreach(println)\n\n dataframe.map(row => {\n    val row1 = row.getAs[String](1)\n    val make = if (row1.toLowerCase == \"tesla\") \"S\" else row1\n    Row(row(0),make,row(2))\n  }).collect().foreach(println)\n\n//[2012,S,S]\n//[1997,Ford,E350]\n//[2015,Chevy,Volt]", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "# \"Moins de 5\" to 0", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat_minor": 0
}